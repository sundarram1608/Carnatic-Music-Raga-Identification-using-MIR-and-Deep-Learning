{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bb5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import Libraries\n",
    "#For Dataframe & Excel handling\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "\n",
    "#For Array manipulations & Audio Processing\n",
    "import numpy as np\n",
    "\n",
    "#For Audio Processing\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "#For File Handling\n",
    "import os\n",
    "\n",
    "#Custom Helper Functions\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4622922",
   "metadata": {},
   "source": [
    "## Sruthi Extraction & Standardization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_df_std_sruthi_merged = run_sruthi_standardization_pipeline()\n",
    "# clipped_df_std_sruthi_merged.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d89a4b",
   "metadata": {},
   "source": [
    "## Sruthi Extraction & Standardiization - Pipeline Individual Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbeba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = get_audio_file_paths()\n",
    "audio_arrays, sampling_rates, original_sruthi_frequency_hz, original_sruthi_frequency_midi, original_sruthi_note = get_sruthi_info(audio_path)\n",
    "audios_standardized_sruthi, std_sruthi_freq_hz, std_sruthi_freq_midi, std_sruthi_note = standardize_sruthi(audio_arrays, sampling_rates, original_sruthi_frequency_hz)\n",
    "# written_paths = save_standardized_audios(audio_path, audios_standardized_sruthi, sampling_rates, out_root=\"sruthi_standardized_audios\", in_root=\"dataset\", suffix=\"_standard_sruthi\")\n",
    "base_dataframe_std_sruthi = create_base_dataframe(audio_path, original_sruthi_note, original_sruthi_frequency_hz, original_sruthi_frequency_midi, std_sruthi_note, std_sruthi_freq_hz, std_sruthi_freq_midi, sampling_rates)\n",
    "clipped_df = split_audio_into_clips(audio_path, audios_standardized_sruthi, sampling_rates, clip_duration_sec=30)\n",
    "clipped_df_std_sruthi_merged = merge_details_to_clip(clipped_df, base_dataframe_std_sruthi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4ac63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of audio_paths: 41\n",
      "Length of audio_arrays: 41\n",
      "Length of sampling_rates: 41\n",
      "Length of original_sruthi_frequency_hz: 41\n",
      "Length of original_sruthi_frequency_midi: 41\n",
      "Length of original_sruthi_note: 41\n",
      "Length of audios_standardized_sruthi: 41\n",
      "Length of base_dataframe_std_sruthi: 41\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of audio_paths: {len(audio_path)}\")\n",
    "print(f\"Length of audio_arrays: {len(audio_arrays)}\")\n",
    "print(f\"Length of sampling_rates: {len(sampling_rates)}\")\n",
    "print(f\"Length of original_sruthi_frequency_hz: {len(original_sruthi_frequency_hz)}\")\n",
    "print(f\"Length of original_sruthi_frequency_midi: {len(original_sruthi_frequency_midi)}\")\n",
    "print(f\"Length of original_sruthi_note: {len(original_sruthi_note)}\")\n",
    "print(f\"Length of audios_standardized_sruthi: {len(audios_standardized_sruthi)}\")\n",
    "# print(f\"Length of written_paths: {len(written_paths)}\")\n",
    "print(f\"Length of base_dataframe_std_sruthi: {len(base_dataframe_std_sruthi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0900d38",
   "metadata": {},
   "source": [
    "## Initial Prototype for Sruthi identification & Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a9344",
   "metadata": {},
   "source": [
    "### Sruthi Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549a735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Sampling Rate: 44100\n",
      "Audio Data Type: <class 'numpy.ndarray'>\n",
      "Audio Data Shape: (44756352,)\n",
      "Audio Duration (seconds): 1014.8832653061224\n",
      "First 10 samples of audio data: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: \n",
    "# Load the audio file and identify the default sampling rate\n",
    "\n",
    "\n",
    "# Load the audio file with the original sampling rate\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Print the default sampling rate\n",
    "print(f\"Default Sampling Rate: {sr}\")\n",
    "print(f\"Audio Data Type: {type(y)}\")\n",
    "print(f\"Audio Data Shape: {y.shape}\")\n",
    "print(f\"Audio Duration (seconds): {librosa.get_duration(y=y, sr=sr)}\")\n",
    "print(f\"First 10 samples of audio data: {y[:10]}\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d9e9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44756352"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048aa648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitches:[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "magnitudes:[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "pitches shape: (1025, 87415)\n",
      "magnitudes shape: (1025, 87415)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Identify the Sruthi (Fundamental Frequency)\n",
    "# Use Harmonic Product Spectrum (HPS) or piptrack to detect pitch\n",
    "# piptrack returns an array of frequencies and their corresponding magnitudes\n",
    "pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
    "\n",
    "print(f\"pitches:{pitches}\")\n",
    "print(f\"magnitudes:{magnitudes}\")\n",
    "print(f\"pitches shape: {pitches.shape}\")\n",
    "print(f\"magnitudes shape: {magnitudes.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decf0f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified Fundamental Frequency (F0): 606.0250854492188 Hz\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the maximum magnitude\n",
    "index = magnitudes.argmax()\n",
    "\n",
    "# Convert the flattened index to 2D indices (row, col)\n",
    "row, col = np.unravel_index(index, magnitudes.shape)\n",
    "\n",
    "# Get the fundamental frequency (F0) from the corresponding pitch\n",
    "f0 = pitches[row, col]\n",
    "\n",
    "print(f\"Identified Fundamental Frequency (F0): {f0} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674e2ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Tonic Note (MIDI): 74.54248760640525\n"
     ]
    }
   ],
   "source": [
    "# Optionally, convert Hz to MIDI note (e.g., for easier pitch comparison)\n",
    "tonic_note_midi = librosa.hz_to_midi(f0)\n",
    "print(f\"Detected Tonic Note (MIDI): {tonic_note_midi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7e3410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fundamental Frequency (F0): 606.0250854492188 Hz\n",
      "Corresponding MIDI Note: 74.54248760640525\n",
      "Musical Pitch Note: Dâ™¯5\n"
     ]
    }
   ],
   "source": [
    "sruthi_note = librosa.midi_to_note(tonic_note_midi)\n",
    "\n",
    "print(f\"Fundamental Frequency (F0): {f0} Hz\")\n",
    "print(f\"Corresponding MIDI Note: {tonic_note_midi}\")\n",
    "print(f\"Musical Pitch Note: {sruthi_note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e540f",
   "metadata": {},
   "source": [
    "### Sruthi Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b369a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch shift steps: -14.542194155055263 semitones\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Standardizing the Sruthi to C4\n",
    "desired_sruthi_freq = 261.63  # Frequency of C4 in Hz\n",
    "\n",
    "# Calculate the pitch shift in semitones\n",
    "# Convert both frequencies to MIDI notes and find the difference in semitones\n",
    "pitch_shift_steps = librosa.hz_to_midi(desired_sruthi_freq) - librosa.hz_to_midi(f0)\n",
    "\n",
    "# Apply pitch shift to the audio (shifting F0 to the desired tonic C4)\n",
    "y_shifted = librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_shift_steps)\n",
    "\n",
    "# Print pitch shift steps\n",
    "print(f\"Pitch shift steps: {pitch_shift_steps} semitones\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f10b3",
   "metadata": {},
   "source": [
    "### Writing Audio to directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab935967",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_shifted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the shifted audio to a new file using soundfile\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mena_gaanu_shifted_audio_to_C4.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43my_shifted\u001b[49m, sr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_shifted' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the shifted audio to a new file using soundfile\n",
    "sf.write('ena_gaanu_shifted_audio_to_C4.wav', y_shifted, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329b713",
   "metadata": {},
   "source": [
    "### Splitting audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44842ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the audio into 30-second clips\n",
    "def split_audio(audio, sr, clip_duration=30):\n",
    "    samples_per_clip = clip_duration * sr\n",
    "    clips = []\n",
    "    \n",
    "    # Split the audio into clips\n",
    "    for start in range(0, len(audio), samples_per_clip):\n",
    "        end = start + samples_per_clip\n",
    "        clip = audio[start:end]  # Extract the clip\n",
    "        if len(clip) == samples_per_clip:  # Only add full-length clips\n",
    "            clips.append(clip)\n",
    "    \n",
    "    return clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74afe833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 30-second clips\n",
    "clips = split_audio(y_shifted, sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_mir)",
   "language": "python",
   "name": "venv_mir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
